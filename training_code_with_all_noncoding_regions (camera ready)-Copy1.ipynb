{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import path \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "directory = os.getcwd()\n",
    "folders = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tonytu/Desktop/all_post_processed_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e2190c9ef3dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/tonytu/Desktop/all_post_processed_data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenomes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.DS_Store'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msorted_genomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tonytu/Desktop/all_post_processed_data/'"
     ]
    }
   ],
   "source": [
    "genomes = [name for name in os.listdir(\"/Users/tonytu/Desktop/all_post_processed_data/\")]\n",
    "genomes.remove('.DS_Store')\n",
    "sorted_genomes = sorted(genomes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tonytu/Desktop/old_stuff/berkeley/fall2022/amiralilab'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_genomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all coding regions and non-coding regions and filter out the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "train_x = np.array([])\n",
    "train_y = np.array([])\n",
    "test_x = np.array([])\n",
    "test_y = np.array([])\n",
    "\n",
    "def flatten(l):\n",
    "    flat_list = [item for sublist in l for item in sublist]\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def read_fasta(dir_name, genome_name, file_name):\n",
    "    #'sampled_genomes_resampled/'\n",
    "    f=open(str(dir_name) + '/' + str(genome_name) + '/' + file_name,'r')\n",
    "    lines=f.readlines()\n",
    "\n",
    "    hre=re.compile('>(\\S+)')\n",
    "    lre=re.compile('^(\\S+)$')\n",
    "\n",
    "    gene={}\n",
    "\n",
    "    for line in lines:\n",
    "            outh = hre.search(line)\n",
    "            if outh:\n",
    "                    id=outh.group(1)\n",
    "            else:\n",
    "                    outl=lre.search(line)\n",
    "                    if(id in gene.keys()):\n",
    "                            gene[id] += outl.group(1)\n",
    "                    else:\n",
    "                            gene[id]  =outl.group(1)\n",
    "    return list(gene.values())\n",
    "\n",
    "def create_csv(dir_name, genome, coding_seqs, sampled_noncoding_seqs):\n",
    "    num_coding_seqs = len(coding_seqs)\n",
    "    num_noncoding_seqs = len(sampled_noncoding_seqs)\n",
    "    coding_indices = np.arange(0, num_coding_seqs, 1)\n",
    "    coding_groundtruth = np.arange(0, num_coding_seqs, 1)\n",
    "    coding_train = ['train'] * num_coding_seqs\n",
    "    noncoding_indices = np.arange(0, num_noncoding_seqs, 1)\n",
    "    noncoding_test = ['test'] * num_noncoding_seqs\n",
    "    noncoding_groundtruth = np.arange(0, num_noncoding_seqs, 1)\n",
    "    \n",
    "    data = {'sequence': coding_seqs, 'target': coding_groundtruth, 'set': coding_train, 'validation': [np.nan] * num_coding_seqs}\n",
    "    coding_df = pd.DataFrame(data, index=coding_indices)\n",
    "    coding_df.to_csv(dir_name + '/' + genome + '/debug_coding_train.csv')\n",
    "    \n",
    "    data = {'sequence': sampled_noncoding_seqs, 'target': noncoding_groundtruth, 'set': noncoding_test, 'validation': [np.nan] * num_noncoding_seqs}\n",
    "    noncoding_df = pd.DataFrame(data, index=noncoding_indices)\n",
    "    noncoding_df.to_csv(dir_name + '/' + genome + '/debug_noncoding_test.csv')\n",
    "    \n",
    "def find_first_index(lst, target):\n",
    "    \"\"\"\n",
    "    Finds the index of the first matching element in the given list.\n",
    "    If no match is found, returns -1.\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(lst):\n",
    "        if item == target:\n",
    "            return i\n",
    "    return -1\n",
    "    \n",
    "def create_dict(lst):\n",
    "    result = {element: [] for element in lst}\n",
    "    return result\n",
    "\n",
    "def filter_duplicate_subdirectories(directories):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where the keys are directory names and the values are lists of subdirectories,\n",
    "    filtered to exclude duplicates except for the first encounter of each subdirectory name\n",
    "    in the order of directories sorted in lexicographic order.\n",
    "    \"\"\"\n",
    "    filtered_directory_indices = create_dict(directories.keys())\n",
    "    filtered_directory_genes = create_dict(directories.keys())\n",
    "    subdirectory_set = set()\n",
    "    for directory_name in sorted(directories.keys()):\n",
    "        for i in range(len(directories[directory_name])):\n",
    "            if not directories[directory_name][i] in subdirectory_set:\n",
    "                filtered_directory_indices[directory_name].append(i)\n",
    "                filtered_directory_genes[directory_name].append(directories[directory_name][i])\n",
    "                subdirectory_set.add(directories[directory_name][i])\n",
    "            else:\n",
    "                continue\n",
    "    return filtered_directory_genes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4288"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = os.listdir('/Users/tonytu/Desktop/old_stuff/berkeley/fall2022/amiralilab/FragGeneScanRs-main/FLIP-main_latest/baselines/embeddings/gb1/esm1b/two_vs_rest/post_processed_data')\n",
    "a.remove('.DS_Store')\n",
    "len(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "coding_genes = []\n",
    "noncoding_genes = []\n",
    "positive_genomes = {}\n",
    "negative_genomes = {}\n",
    "\n",
    "#note that all the coding regions are from the previous data directory (root_dir)\n",
    "#all the noncoding regions are from the new data directory: all_post_processed_data\n",
    "coding_regions_data_dir_path = root_dir\n",
    "noncoding_regions_data_dir_path = '/Users/tonytu/Desktop/all_post_processed_data_old/'\n",
    "\n",
    "for genome in genomes:\n",
    "    # adding a length constraint\n",
    "    length_upper_limit = 60\n",
    "    length_lower_limit = 0\n",
    "    positive_testing_sequences = list(pd.read_csv(coding_regions_data_dir_path + genome + '/coding_train.csv')['sequence'])\n",
    "    negative_testing_sequences = list(pd.read_csv(noncoding_regions_data_dir_path + genome + '/all_noncoding_test.csv')['sequence'])\n",
    "    positive_testing_sequences = [positive_testing_sequence for positive_testing_sequence in positive_testing_sequences if len(positive_testing_sequence) < 60]\n",
    "    negative_testing_sequences = [negative_testing_sequence for negative_testing_sequence in negative_testing_sequences if len(negative_testing_sequence) < 60]\n",
    "    coding_genes.extend(positive_testing_sequences)\n",
    "    noncoding_genes.extend(negative_testing_sequences)\n",
    "    \n",
    "    positive_genomes[genome] = positive_testing_sequences\n",
    "    negative_genomes[genome] = negative_testing_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 145232 coding regions, out of which 128615 are unique.\n",
      "In total, there are 3367378 noncoding regions, out of which 3367378 are unique.\n"
     ]
    }
   ],
   "source": [
    "print('In total, there are ' + str(len(coding_genes)) + ' coding regions, out of which ' + str(len(set(coding_genes))) + ' are unique.')\n",
    "print('In total, there are ' + str(len(noncoding_genes)) + ' noncoding regions, out of which ' + str(len(set(noncoding_genes))) + ' are unique.')\n",
    "\n",
    "#all noncoding_regions are already unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "result = collections.Counter(coding_genes) & collections.Counter(noncoding_genes)\n",
    "intersected_list = list(result.elements())\n",
    "print(len(intersected_list))\n",
    "print(len(set(intersected_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing these from all datasets\n",
    "unique_intersected_list = set(intersected_list)\n",
    "for genome in genomes:\n",
    "    for gene in positive_genomes[genome]:\n",
    "        if gene in unique_intersected_list:\n",
    "            positive_genomes[genome].remove(gene)\n",
    "    for gene in negative_genomes[genome]:\n",
    "        if gene in unique_intersected_list:\n",
    "            negative_genomes[genome].remove(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genome in genomes:\n",
    "    for gene in positive_genomes[genome]:\n",
    "        if len(gene) >= 60:\n",
    "            positive_genomes[genome].remove(gene)\n",
    "    for gene in negative_genomes[genome]:\n",
    "        if len(gene) >= 60:\n",
    "            negative_genomes[genome].remove(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicate_subdirectories(directories):\n",
    "    \"\"\"\n",
    "    Returns a dictionary where the keys are directory names and the values are lists of subdirectories,\n",
    "    filtered to exclude duplicates except for the first encounter of each subdirectory name\n",
    "    in the order of directories sorted in lexicographic order.\n",
    "    \"\"\"\n",
    "    filtered_directory_indices = create_dict(directories.keys())\n",
    "    filtered_directory_genes = create_dict(directories.keys())\n",
    "    subdirectory_set = set()\n",
    "    for directory_name in sorted(directories.keys()):\n",
    "        for i in range(len(directories[directory_name])):\n",
    "            if not directories[directory_name][i] in subdirectory_set:\n",
    "                filtered_directory_indices[directory_name].append(i)\n",
    "                filtered_directory_genes[directory_name].append(directories[directory_name][i])\n",
    "                subdirectory_set.add(directories[directory_name][i])\n",
    "            else:\n",
    "                continue\n",
    "    return filtered_directory_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running filtering on all genomes\n",
    "positive_filtered_genes = filter_duplicate_subdirectories(positive_genomes)\n",
    "negative_filtered_genes = filter_duplicate_subdirectories(negative_genomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 intersected sequences\n"
     ]
    }
   ],
   "source": [
    "#sanity check that there are no duplicates and no shared elements in coding regions and noncoding regions\n",
    "print('There are ' + str(len(collections.Counter(flatten(positive_filtered_genes.values())) & collections.Counter(flatten(negative_filtered_genes.values())))) + ' intersected sequences')\n",
    "assert(len(flatten(positive_filtered_genes.values())) == len(set(flatten(positive_filtered_genes.values()))))\n",
    "assert(len(flatten(negative_filtered_genes.values())) == len(set(flatten(negative_filtered_genes.values()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are, in total, 128614 coding regions\n",
      "There are, in total, 3367377 noncoding regions\n"
     ]
    }
   ],
   "source": [
    "#data after filtering\n",
    "print('There are, in total, ' + str(len(flatten(positive_filtered_genes.values()))) + ' coding regions')\n",
    "print('There are, in total, ' + str(len(flatten(negative_filtered_genes.values()))) + ' noncoding regions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the indices:\n",
    "positive_filtered_genes_indices = {}\n",
    "negative_filtered_genes_indices = {}\n",
    "\n",
    "def indices_of_elements(list1, list2):\n",
    "    indices = []\n",
    "    for element in list1:\n",
    "        try:\n",
    "            index = list2.index(element)\n",
    "            indices.append(index)\n",
    "        except ValueError:\n",
    "            print('sequence not found!')\n",
    "            pass\n",
    "    return indices\n",
    "\n",
    "for genome in genomes:\n",
    "    positive_testing_sequences = list(pd.read_csv(coding_regions_data_dir_path + genome + '/coding_train.csv')['sequence'])\n",
    "    negative_testing_sequences = list(pd.read_csv(noncoding_regions_data_dir_path + genome + '/all_noncoding_test.csv')['sequence'])\n",
    "    \n",
    "    filtered_positive_genes = positive_filtered_genes[genome]\n",
    "    filtered_negative_genes = negative_filtered_genes[genome]\n",
    "    \n",
    "    positive_filtered_genes_indices[genome] = indices_of_elements(filtered_positive_genes, positive_testing_sequences)\n",
    "    negative_filtered_genes_indices[genome] = indices_of_elements(filtered_negative_genes, negative_testing_sequences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting train and test folders:\n",
    "MAX_FOLD = 10\n",
    "post_processed_data_PATH = '/Users/tonytu/Desktop/old_stuff/berkeley/fall2022/amiralilab/post_processed_data'\n",
    "all_genomes = os.listdir(post_processed_data_PATH)\n",
    "all_genomes.remove('.DS_Store')\n",
    "\n",
    "\n",
    "def retrieve_train_test_folders(genomes, hold_out_index):\n",
    "    num_genomes = len(genomes)\n",
    "    if hold_out_index == MAX_FOLD - 1:\n",
    "        test_split = genomes[int((num_genomes//10) * hold_out_index): ]\n",
    "        train_split = list(set(genomes) - set(test_split))\n",
    "        assert len(train_split) + len(test_split) == num_genomes, \"numbers do not match up!\"\n",
    "        return train_split, test_split\n",
    "    test_split = genomes[int((num_genomes//10) * hold_out_index): int(num_genomes//10 * (hold_out_index + 1))]\n",
    "    train_split = list(set(genomes) - set(test_split))\n",
    "    assert len(train_split) + len(test_split) == num_genomes, \"numbers do not match up!\"\n",
    "    return train_split, test_split\n",
    "    \n",
    "# train_genomes, test_genomes = retrieve_train_test_folders(all_genomes, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "def load_data(genomes):\n",
    "    all_x = np.array([])\n",
    "    all_y = np.array([])\n",
    "\n",
    "    random.seed(5)\n",
    "    torch.manual_seed(5)\n",
    "    for genome in genomes:\n",
    "        \n",
    "        positive_indices = np.array(positive_filtered_genes_indices[genome]) \n",
    "        negative_indices = np.array(negative_filtered_genes_indices[genome])\n",
    "        if positive_indices.shape[0] != 0:\n",
    "            genome_train_coding_embeddings = torch.load(coding_regions_data_dir_path + genome + '/coding_train_emb/train_mean.pt')\n",
    "            genome_train_coding_labels = np.ones(genome_train_coding_embeddings.shape[0])[positive_indices]\n",
    "            genome_train_coding_embeddings = genome_train_coding_embeddings[positive_indices]\n",
    "            assert genome_train_coding_embeddings.shape[0] == len(positive_indices)\n",
    "\n",
    "        if negative_indices.shape[0] != 0:\n",
    "            genome_train_noncoding_embeddings = torch.load(noncoding_regions_data_dir_path+ genome + '/noncoding_test_emb/test_mean.pt')\n",
    "            genome_train_noncoding_labels = np.zeros(genome_train_noncoding_embeddings.shape[0])[negative_indices]\n",
    "            genome_train_noncoding_embeddings = genome_train_noncoding_embeddings[negative_indices]\n",
    "            assert genome_train_noncoding_embeddings.shape[0] == len(negative_indices)\n",
    "\n",
    "        if all_x.shape[0] == 0:\n",
    "            all_x = genome_train_coding_embeddings\n",
    "            all_y = genome_train_coding_labels\n",
    "            all_x = np.concatenate((all_x, genome_train_noncoding_embeddings), axis=0)\n",
    "            all_y = np.concatenate((all_y, genome_train_noncoding_labels), axis=0)\n",
    "            continue\n",
    "\n",
    "        if positive_indices.shape[0] != 0:\n",
    "            all_x = np.concatenate((all_x, genome_train_coding_embeddings), axis=0)\n",
    "            all_y = np.concatenate((all_y, genome_train_coding_labels), axis=0)\n",
    "        \n",
    "        if negative_indices.shape[0] != 0:\n",
    "            all_x = np.concatenate((all_x, genome_train_noncoding_embeddings), axis=0)\n",
    "            all_y = np.concatenate((all_y, genome_train_noncoding_labels), axis=0)\n",
    "    return all_x, all_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check that the dimension of the loaded data is equal to the dimension of the indices in the dictionary\n",
    "assert np.vstack(all_data_folds_embs).shape[0] == len(flatten(positive_filtered_genes.values())) + len(flatten(negative_filtered_genes.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading classifier:\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super(Net,self).__init__()\n",
    "    #input embedding: 1280\n",
    "    self.fc1 = nn.Linear(input_shape,32)\n",
    "    self.fc2 = nn.Linear(32,64)\n",
    "    self.fc3 = nn.Linear(64,1)\n",
    "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.sigmoid(self.fc3(x))\n",
    "    return x\n",
    "\n",
    "class Net1(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super(Net1,self).__init__()\n",
    "    #input embedding: 1280\n",
    "    self.fc1 = nn.Linear(input_shape,32)\n",
    "    self.fc2 = nn.Linear(32,64)\n",
    "    self.fc3 = nn.Linear(64,128)\n",
    "    self.fc4 = nn.Linear(128,64)\n",
    "    self.fc5 = nn.Linear(64,32)\n",
    "    self.fc6 = nn.Linear(32,1)\n",
    "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    x = torch.relu(self.fc5(x))\n",
    "    x = torch.sigmoid(self.fc6(x))\n",
    "    return x\n",
    "\n",
    "class Net3(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super(Net3,self).__init__()\n",
    "    #input embedding: 1280\n",
    "    self.fc1 = nn.Linear(input_shape,1)\n",
    "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    x = torch.sigmoid(self.fc1(x))\n",
    "    return x\n",
    "\n",
    "class Net4(nn.Module):\n",
    "  def __init__(self,input_shape):\n",
    "    super(Net4,self).__init__()\n",
    "    #input embedding: 1280\n",
    "    self.fc1 = nn.Linear(input_shape,128)\n",
    "    self.fc2 = nn.Linear(128,256)\n",
    "    self.fc3 = nn.Linear(256,512)\n",
    "    self.fc4 = nn.Linear(512,256)\n",
    "    self.fc5 = nn.Linear(256,128)\n",
    "    self.fc6 = nn.Linear(128,64)\n",
    "    self.fc7 = nn.Linear(64,32)\n",
    "    self.fc8 = nn.Linear(32,1)\n",
    "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc7.weight)\n",
    "    torch.nn.init.xavier_uniform_(self.fc8.weight)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    x = torch.relu(self.fc5(x))\n",
    "    x = torch.relu(self.fc6(x))\n",
    "    x = torch.relu(self.fc7(x))\n",
    "    x = torch.sigmoid(self.fc8(x))\n",
    "    return x\n",
    "\n",
    "def flatten(l):\n",
    "    flat_list = [item for sublist in l for item in sublist]\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3157371, 1280)\n",
      "(3157371,)\n",
      "(338620, 1280)\n",
      "(338620,)\n"
     ]
    }
   ],
   "source": [
    "def create_train_test_data(all_data_folds_embs, all_data_folds_labels, fold):\n",
    "    test_split = [fold]\n",
    "    train_split = list(set(list(range(10))) - set([fold]))\n",
    "    train_x = np.vstack([all_data_folds_embs[i] for i in train_split])\n",
    "    train_y = np.concatenate([all_data_folds_labels[i] for i in train_split])\n",
    "    test_x = np.vstack([all_data_folds_embs[i] for i in test_split])\n",
    "    test_y = np.concatenate([all_data_folds_labels[i] for i in test_split])\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "train_x, train_y, test_x, test_y = create_train_test_data(all_data_folds_embs, all_data_folds_labels, 0)\n",
    "\n",
    "#checking dimensions:\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly shuffling train_x, train_y and their corresponding labels\n",
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "training_indices = np.arange(train_x.shape[0])\n",
    "test_indices = np.arange(test_x.shape[0])\n",
    "np.random.shuffle(training_indices)\n",
    "np.random.shuffle(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[training_indices]\n",
    "train_y = train_y[training_indices]\n",
    "test_x = test_x[test_indices]\n",
    "test_y = test_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class dataset(Dataset):\n",
    "  def __init__(self,x,y):\n",
    "    self.x = torch.tensor(x,dtype=torch.float32)\n",
    "    self.y = torch.tensor(y,dtype=torch.float32)\n",
    "    self.length = self.x.shape[0]\n",
    " \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x[idx],self.y[idx]\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "trainset = dataset(train_x, train_y)\n",
    "#DataLoader\n",
    "trainloader = DataLoader(trainset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "learning_rate = 0.01\n",
    "epochs = 300\n",
    "# Model , Optimizer, Loss\n",
    "ecoli_model = Net4(input_shape=train_x.shape[1])\n",
    "optimizer = torch.optim.SGD(ecoli_model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward loop\n",
    "random.seed(5)\n",
    "torch.manual_seed(5)\n",
    "losses = []\n",
    "accur = []\n",
    "for i in range(epochs):\n",
    "  for j,(x_train,y_train) in enumerate(trainloader):\n",
    "    \n",
    "    #calculate output\n",
    "    output = ecoli_model(x_train)\n",
    " \n",
    "    #calculate loss\n",
    "    loss = loss_fn(output,y_train.reshape(-1,1))\n",
    " \n",
    "    #accuracy\n",
    "    predicted = ecoli_model(torch.tensor(x_train,dtype=torch.float32))\n",
    "    acc = (predicted.reshape(-1).detach().numpy().round() == np.array(y_train)).mean()\n",
    "    #backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  if i%50 == 0:\n",
    "    losses.append(loss)\n",
    "    accur.append(acc)\n",
    "    print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 10 fold cross validation on all coding + noncoding regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonytu/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\tloss : 0.02037082053720951\t accuracy : 1.0\n",
      "epoch 50\tloss : 0.014096093364059925\t accuracy : 1.0\n",
      "epoch 100\tloss : 0.010964148677885532\t accuracy : 1.0\n",
      "epoch 150\tloss : 0.008270354010164738\t accuracy : 1.0\n",
      "epoch 200\tloss : 0.0068092867732048035\t accuracy : 1.0\n",
      "epoch 250\tloss : 0.007393664214760065\t accuracy : 1.0\n",
      "epoch 0\tloss : 0.09614776819944382\t accuracy : 0.9615384615384616\n",
      "epoch 50\tloss : 0.11876317113637924\t accuracy : 0.9615384615384616\n",
      "epoch 100\tloss : 0.11171449720859528\t accuracy : 0.9615384615384616\n",
      "epoch 150\tloss : 0.10179932415485382\t accuracy : 0.9615384615384616\n",
      "epoch 200\tloss : 0.11020828038454056\t accuracy : 0.9615384615384616\n",
      "epoch 250\tloss : 0.08225217461585999\t accuracy : 0.9615384615384616\n",
      "epoch 0\tloss : 0.0177464596927166\t accuracy : 1.0\n",
      "epoch 50\tloss : 0.0021531996317207813\t accuracy : 1.0\n",
      "epoch 100\tloss : 0.0021889079362154007\t accuracy : 1.0\n",
      "epoch 150\tloss : 0.0014818101190030575\t accuracy : 1.0\n",
      "epoch 200\tloss : 0.0011308063985779881\t accuracy : 1.0\n",
      "epoch 250\tloss : 0.0008491728804074228\t accuracy : 1.0\n",
      "epoch 0\tloss : 0.26907292008399963\t accuracy : 0.95\n",
      "epoch 50\tloss : 0.26717859506607056\t accuracy : 0.9\n",
      "epoch 100\tloss : 0.20462074875831604\t accuracy : 0.9\n",
      "epoch 150\tloss : 0.13766750693321228\t accuracy : 0.95\n",
      "epoch 200\tloss : 0.07733608037233353\t accuracy : 0.95\n",
      "epoch 250\tloss : 0.07062695920467377\t accuracy : 0.95\n",
      "epoch 0\tloss : 0.004004755523055792\t accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 10, 1):\n",
    "    train_x, train_y, test_x, test_y = create_train_test_data(all_data_folds_embs, all_data_folds_labels, epoch)\n",
    "    \n",
    "    random.seed(5)\n",
    "    torch.manual_seed(5)\n",
    "    training_indices = np.arange(train_x.shape[0])\n",
    "    test_indices = np.arange(test_x.shape[0])\n",
    "    np.random.shuffle(training_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    train_x = train_x[training_indices]\n",
    "    train_y = train_y[training_indices]\n",
    "    test_x = test_x[test_indices]\n",
    "    test_y = test_y[test_indices]\n",
    "    \n",
    "    trainset = dataset(train_x, train_y)\n",
    "    #DataLoader\n",
    "    trainloader = DataLoader(trainset,batch_size=64,shuffle=False)\n",
    "    \n",
    "    #hyper parameters\n",
    "    random.seed(5)\n",
    "    torch.manual_seed(5)\n",
    "    learning_rate = 0.01\n",
    "    epochs = 300\n",
    "    # Model , Optimizer, Loss\n",
    "    ecoli_model = Net1(input_shape=train_x.shape[1])\n",
    "    optimizer = torch.optim.SGD(ecoli_model.parameters(),lr=learning_rate)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    random.seed(5)\n",
    "    torch.manual_seed(5)\n",
    "    losses = []\n",
    "    accur = []\n",
    "    for i in range(epochs):\n",
    "      for j,(x_train,y_train) in enumerate(trainloader):\n",
    "\n",
    "        #calculate output\n",
    "        output = ecoli_model(x_train)\n",
    "\n",
    "        #calculate loss\n",
    "        loss = loss_fn(output,y_train.reshape(-1,1))\n",
    "\n",
    "        #accuracy\n",
    "        predicted = ecoli_model(torch.tensor(x_train,dtype=torch.float32))\n",
    "        acc = (predicted.reshape(-1).detach().numpy().round() == np.array(y_train)).mean()\n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      if i%50 == 0:\n",
    "        losses.append(loss)\n",
    "        accur.append(acc)\n",
    "        print(\"epoch {}\\tloss : {}\\t accuracy : {}\".format(i,loss,acc))\n",
    "    \n",
    "    #saving model\n",
    "    PATH = \"/Users/tonytu/desktop/old_stuff/berkeley/fall2022/amiralilab/all_available_data_final_ten_fold_cross_validation_models/six_layer_fold_\" + str(epoch) + \"_genomes.pt\"\n",
    "    torch.save(ecoli_model, PATH)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
